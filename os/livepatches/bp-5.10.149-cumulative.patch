diff --git a/arch/x86/kernel/cpu/proc.c b/arch/x86/kernel/cpu/proc.c
index 4eec8889b0ff..f3ea0b35e82b 100644
--- a/arch/x86/kernel/cpu/proc.c
+++ b/arch/x86/kernel/cpu/proc.c
@@ -4,6 +4,7 @@
 #include <linux/string.h>
 #include <linux/seq_file.h>
 #include <linux/cpufreq.h>
+#include <linux/user_namespace.h>
 
 #include "cpu.h"
 
@@ -58,14 +59,52 @@ static void show_cpuinfo_misc(struct seq_file *m, struct cpuinfo_x86 *c)
 }
 #endif
 
+struct mutex *show_cpuinfo_cache_mutexes[NR_CPUS] = { 0 };
+unsigned long show_cpuinfo_cache_jiffies[NR_CPUS] = { 0 };
+char *show_cpuinfo_cache[NR_CPUS] = { 0 };
+
 static int show_cpuinfo(struct seq_file *m, void *v)
 {
 	struct cpuinfo_x86 *c = v;
 	unsigned int cpu;
 	int i;
+	unsigned long now = jiffies;
+	struct seq_file *dupm;
 
 	cpu = c->cpu_index;
-	seq_printf(m, "processor\t: %u\n"
+
+	if (!show_cpuinfo_cache_mutexes[cpu]) {
+		show_cpuinfo_cache_mutexes[cpu] = kmalloc(sizeof(struct mutex), GFP_KERNEL);
+		if (!show_cpuinfo_cache_mutexes[cpu])
+			return -ERESTARTSYS;
+		mutex_init(show_cpuinfo_cache_mutexes[cpu]);
+	}
+
+	mutex_lock(show_cpuinfo_cache_mutexes[cpu]);
+	if (now - show_cpuinfo_cache_jiffies[cpu] > msecs_to_jiffies(5000)) {
+		if (show_cpuinfo_cache[cpu])
+			kfree(show_cpuinfo_cache[cpu]);
+	} else if (show_cpuinfo_cache[cpu]) {
+		seq_puts(m, show_cpuinfo_cache[cpu]);
+		mutex_unlock(show_cpuinfo_cache_mutexes[cpu]);
+		return 0;
+	}
+
+	dupm = kzalloc(sizeof(struct seq_file), GFP_KERNEL);
+	if (!dupm) {
+		mutex_unlock(show_cpuinfo_cache_mutexes[cpu]);
+		return -ENOMEM;
+	}
+	dupm->buf = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!dupm->buf) {
+		kfree(dupm);
+		mutex_unlock(show_cpuinfo_cache_mutexes[cpu]);
+		return -ENOMEM;
+	}
+	dupm->size = PAGE_SIZE;
+	mutex_init(&dupm->lock);
+
+	seq_printf(dupm, "processor\t: %u\n"
 		   "vendor_id\t: %s\n"
 		   "cpu family\t: %d\n"
 		   "model\t\t: %u\n"
@@ -77,11 +116,11 @@ static int show_cpuinfo(struct seq_file *m, void *v)
 		   c->x86_model_id[0] ? c->x86_model_id : "unknown");
 
 	if (c->x86_stepping || c->cpuid_level >= 0)
-		seq_printf(m, "stepping\t: %d\n", c->x86_stepping);
+		seq_printf(dupm, "stepping\t: %d\n", c->x86_stepping);
 	else
-		seq_puts(m, "stepping\t: unknown\n");
+		seq_puts(dupm, "stepping\t: unknown\n");
 	if (c->microcode)
-		seq_printf(m, "microcode\t: 0x%x\n", c->microcode);
+		seq_printf(dupm, "microcode\t: 0x%x\n", c->microcode);
 
 	if (cpu_has(c, X86_FEATURE_TSC)) {
 		unsigned int freq = aperfmperf_get_khz(cpu);
@@ -90,69 +129,81 @@ static int show_cpuinfo(struct seq_file *m, void *v)
 			freq = cpufreq_quick_get(cpu);
 		if (!freq)
 			freq = cpu_khz;
-		seq_printf(m, "cpu MHz\t\t: %u.%03u\n",
+		seq_printf(dupm, "cpu MHz\t\t: %u.%03u\n",
 			   freq / 1000, (freq % 1000));
 	}
 
 	/* Cache size */
 	if (c->x86_cache_size)
-		seq_printf(m, "cache size\t: %u KB\n", c->x86_cache_size);
+		seq_printf(dupm, "cache size\t: %u KB\n", c->x86_cache_size);
 
-	show_cpuinfo_core(m, c, cpu);
-	show_cpuinfo_misc(m, c);
+	show_cpuinfo_core(dupm, c, cpu);
+	show_cpuinfo_misc(dupm, c);
 
-	seq_puts(m, "flags\t\t:");
+	seq_puts(dupm, "flags\t\t:");
 	for (i = 0; i < 32*NCAPINTS; i++)
 		if (cpu_has(c, i) && x86_cap_flags[i] != NULL)
-			seq_printf(m, " %s", x86_cap_flags[i]);
+			seq_printf(dupm, " %s", x86_cap_flags[i]);
 
 #ifdef CONFIG_X86_VMX_FEATURE_NAMES
 	if (cpu_has(c, X86_FEATURE_VMX) && c->vmx_capability[0]) {
-		seq_puts(m, "\nvmx flags\t:");
+		seq_puts(dupm, "\nvmx flags\t:");
 		for (i = 0; i < 32*NVMXINTS; i++) {
 			if (test_bit(i, (unsigned long *)c->vmx_capability) &&
 			    x86_vmx_flags[i] != NULL)
-				seq_printf(m, " %s", x86_vmx_flags[i]);
+				seq_printf(dupm, " %s", x86_vmx_flags[i]);
 		}
 	}
 #endif
 
-	seq_puts(m, "\nbugs\t\t:");
+	seq_puts(dupm, "\nbugs\t\t:");
 	for (i = 0; i < 32*NBUGINTS; i++) {
 		unsigned int bug_bit = 32*NCAPINTS + i;
 
 		if (cpu_has_bug(c, bug_bit) && x86_bug_flags[i])
-			seq_printf(m, " %s", x86_bug_flags[i]);
+			seq_printf(dupm, " %s", x86_bug_flags[i]);
 	}
 
-	seq_printf(m, "\nbogomips\t: %lu.%02lu\n",
+	seq_printf(dupm, "\nbogomips\t: %lu.%02lu\n",
 		   c->loops_per_jiffy/(500000/HZ),
 		   (c->loops_per_jiffy/(5000/HZ)) % 100);
 
 #ifdef CONFIG_X86_64
 	if (c->x86_tlbsize > 0)
-		seq_printf(m, "TLB size\t: %d 4K pages\n", c->x86_tlbsize);
+		seq_printf(dupm, "TLB size\t: %d 4K pages\n", c->x86_tlbsize);
 #endif
-	seq_printf(m, "clflush size\t: %u\n", c->x86_clflush_size);
-	seq_printf(m, "cache_alignment\t: %d\n", c->x86_cache_alignment);
-	seq_printf(m, "address sizes\t: %u bits physical, %u bits virtual\n",
+	seq_printf(dupm, "clflush size\t: %u\n", c->x86_clflush_size);
+	seq_printf(dupm, "cache_alignment\t: %d\n", c->x86_cache_alignment);
+	seq_printf(dupm, "address sizes\t: %u bits physical, %u bits virtual\n",
 		   c->x86_phys_bits, c->x86_virt_bits);
 
-	seq_puts(m, "power management:");
+	seq_puts(dupm, "power management:");
 	for (i = 0; i < 32; i++) {
 		if (c->x86_power & (1 << i)) {
 			if (i < ARRAY_SIZE(x86_power_flags) &&
 			    x86_power_flags[i])
-				seq_printf(m, "%s%s",
+				seq_printf(dupm, "%s%s",
 					   x86_power_flags[i][0] ? " " : "",
 					   x86_power_flags[i]);
 			else
-				seq_printf(m, " [%d]", i);
+				seq_printf(dupm, " [%d]", i);
 		}
 	}
 
-	seq_puts(m, "\n\n");
+	seq_puts(dupm, "\n\n");
 
+	if (dupm->buf) {
+		show_cpuinfo_cache[cpu] = kzalloc(dupm->count + 1, GFP_KERNEL);
+		if (show_cpuinfo_cache[cpu])
+			memcpy(show_cpuinfo_cache[cpu], dupm->buf, dupm->count);
+		seq_puts(m, dupm->buf);
+		kfree(dupm->buf);
+	}
+	if (show_cpuinfo_cache[cpu]) {
+		show_cpuinfo_cache_jiffies[cpu] = now;
+	}
+	kfree(dupm);
+	mutex_unlock(show_cpuinfo_cache_mutexes[cpu]);
 	return 0;
 }
 
diff --git a/drivers/net/vrf.c b/drivers/net/vrf.c
index 8ab0b5a8dfef..546bb6f9dd76 100644
--- a/drivers/net/vrf.c
+++ b/drivers/net/vrf.c
@@ -1433,14 +1433,12 @@ static struct sk_buff *vrf_l3_rcv(struct net_device *vrf_dev,
 #if IS_ENABLED(CONFIG_IPV6)
 /* send to link-local or multicast address via interface enslaved to
  * VRF device. Force lookup to VRF table without changing flow struct
- * Note: Caller to this function must hold rcu_read_lock() and no refcnt
- * is taken on the dst by this function.
  */
 static struct dst_entry *vrf_link_scope_lookup(const struct net_device *dev,
 					      struct flowi6 *fl6)
 {
 	struct net *net = dev_net(dev);
-	int flags = RT6_LOOKUP_F_IFACE | RT6_LOOKUP_F_DST_NOREF;
+	int flags = RT6_LOOKUP_F_IFACE;
 	struct dst_entry *dst = NULL;
 	struct rt6_info *rt;
 
@@ -1450,6 +1448,7 @@ static struct dst_entry *vrf_link_scope_lookup(const struct net_device *dev,
 	 */
 	if (fl6->flowi6_oif == dev->ifindex) {
 		dst = &net->ipv6.ip6_null_entry->dst;
+		dst_hold(dst);
 		return dst;
 	}
 
diff --git a/fs/proc/cpuinfo.c b/fs/proc/cpuinfo.c
index 419760fd77bd..7a42ced1aa45 100644
--- a/fs/proc/cpuinfo.c
+++ b/fs/proc/cpuinfo.c
@@ -9,10 +9,19 @@ __weak void arch_freq_prepare_all(void)
 {
 }
 
+extern unsigned long show_cpuinfo_cache_jiffies[NR_CPUS];
 extern const struct seq_operations cpuinfo_op;
 static int cpuinfo_open(struct inode *inode, struct file *file)
 {
-	arch_freq_prepare_all();
+	unsigned long now = jiffies;
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		if (now - show_cpuinfo_cache_jiffies[cpu] > msecs_to_jiffies(5000)) {
+			arch_freq_prepare_all();
+			break;
+		}
+	}
 	return seq_open(file, &cpuinfo_op);
 }
 
diff --git a/kernel/cgroup/cgroup.c b/kernel/cgroup/cgroup.c
index a4325ebc6979..bb58d3041349 100644
--- a/kernel/cgroup/cgroup.c
+++ b/kernel/cgroup/cgroup.c
@@ -68,6 +68,8 @@
 /* let's not notify more than 100 times per second */
 #define CGROUP_FILE_NOTIFY_MIN_INTV	DIV_ROUND_UP(HZ, 100)
 
+void proc_cgroup_cache_clear(struct task_struct *tsk);
+
 /*
  * cgroup_mutex is the master lock.  Any modification to cgroup or its
  * hierarchy must be performed while holding it.
@@ -855,6 +857,12 @@ static void css_set_skip_task_iters(struct css_set *cset,
 		css_task_iter_skip(it, task);
 }
 
+#include <linux/livepatch.h>
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
 /**
  * css_set_move_task - move a task from one css_set to another
  * @task: task being moved
@@ -5955,6 +5963,71 @@ void cgroup_path_from_kernfs_id(u64 id, char *buf, size_t buflen)
 	kernfs_put(kn);
 }
 
+/* Needs tsk->proc_cgroup_mutex */
+void proc_cgroup_cache_clear(struct task_struct *tsk)
+{
+	struct mutex *mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+
+	if (!mutex || !caches || !keys)
+		return;
+
+	for (i = 0; i < 16; i++) {
+		if (keys[i] != 0) {
+			keys[i] = 0;
+			if (caches[i])
+				kfree(caches[i]);
+		};
+	};
+}
+
+/* Needs tsk->proc_cgroup_mutex */
+char *proc_cgroup_cache_lookup(struct task_struct *tsk, struct cgroup_namespace *srchkey)
+{
+	struct mutex *mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+
+	if (!mutex || !caches || !keys)
+		return NULL;
+
+	for (i = 0; i < 16; i++) {
+		if (keys[i] == srchkey)
+			return caches[i];
+	};
+	return NULL;
+}
+
+/* Needs tsk->proc_cgroup_mutex */
+char *proc_cgroup_cache_alloc(struct task_struct *tsk, struct cgroup_namespace *srchkey, char* buf, size_t len)
+{
+	struct mutex *mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+	char *ret;
+
+	if (!mutex || !caches || !keys)
+		return NULL;
+
+	for (i = 0; i < 16; i++) {
+		if (!keys[i]) {
+			ret = kzalloc(len+1, GFP_KERNEL);
+			if (!ret)
+				return NULL;
+			caches[i] = ret;
+			keys[i] = srchkey;
+			memcpy(ret, buf, len);
+			return ret;
+		};
+	};
+	proc_cgroup_cache_clear(tsk);
+	return NULL;
+}
+
 /*
  * proc_cgroup_show()
  *  - Print task's cgroup paths into seq_file, one line for each hierarchy
@@ -5963,11 +6036,37 @@ void cgroup_path_from_kernfs_id(u64 id, char *buf, size_t buflen)
 int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 		     struct pid *pid, struct task_struct *tsk)
 {
+	char *cache;
 	char *buf;
-	int retval;
+	int retval = -ENOMEM;
 	struct cgroup_root *root;
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+	struct seq_file *dupm;
+
+	if (!proc_cgroup_mutex)
+		goto orig;
+	mutex_lock(proc_cgroup_mutex);
+	cache = proc_cgroup_cache_lookup(tsk, current->nsproxy->cgroup_ns);
+	if (cache) {
+		seq_puts(m, cache);
+		mutex_unlock(proc_cgroup_mutex);
+		return 0;
+	}
 
-	retval = -ENOMEM;
+	dupm = kzalloc(sizeof(struct seq_file), GFP_KERNEL);
+	if (!dupm) {
+		mutex_unlock(proc_cgroup_mutex);
+		return -ENOMEM;
+	}
+	dupm->buf = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!dupm->buf) {
+		kfree(dupm);
+		mutex_unlock(proc_cgroup_mutex);
+		return -ENOMEM;
+	}
+	dupm->size = PAGE_SIZE;
+	mutex_init(&dupm->lock);
+orig:
 	buf = kmalloc(PATH_MAX, GFP_KERNEL);
 	if (!buf)
 		goto out;
@@ -5983,16 +6082,16 @@ int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 		if (root == &cgrp_dfl_root && !cgrp_dfl_visible)
 			continue;
 
-		seq_printf(m, "%d:", root->hierarchy_id);
+		seq_printf(dupm, "%d:", root->hierarchy_id);
 		if (root != &cgrp_dfl_root)
 			for_each_subsys(ss, ssid)
 				if (root->subsys_mask & (1 << ssid))
-					seq_printf(m, "%s%s", count++ ? "," : "",
+					seq_printf(dupm, "%s%s", count++ ? "," : "",
 						   ss->legacy_name);
 		if (strlen(root->name))
-			seq_printf(m, "%sname=%s", count ? "," : "",
+			seq_printf(dupm, "%sname=%s", count ? "," : "",
 				   root->name);
-		seq_putc(m, ':');
+		seq_putc(dupm, ':');
 
 		cgrp = task_cgroup_from_root(tsk, root);
 
@@ -6013,15 +6112,15 @@ int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 			if (retval < 0)
 				goto out_unlock;
 
-			seq_puts(m, buf);
+			seq_puts(dupm, buf);
 		} else {
-			seq_puts(m, "/");
+			seq_puts(dupm, "/");
 		}
 
 		if (cgroup_on_dfl(cgrp) && cgroup_is_dead(cgrp))
-			seq_puts(m, " (deleted)\n");
+			seq_puts(dupm, " (deleted)\n");
 		else
-			seq_putc(m, '\n');
+			seq_putc(dupm, '\n');
 	}
 
 	retval = 0;
@@ -6030,6 +6129,17 @@ int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 	mutex_unlock(&cgroup_mutex);
 	kfree(buf);
 out:
+	if (!proc_cgroup_mutex)
+		goto out_orig;
+	if (dupm->buf) {
+		cache = proc_cgroup_cache_alloc(tsk, current->nsproxy->cgroup_ns, dupm->buf, dupm->count);
+		kfree(dupm->buf);
+	}
+	if (cache)
+		seq_puts(m, cache);
+	kfree(dupm);
+	mutex_unlock(proc_cgroup_mutex);
+out_orig:
 	return retval;
 }
 
diff --git a/kernel/fork.c b/kernel/fork.c
index 6a060869f94c..1444f458bfa7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -439,6 +439,19 @@ void put_task_stack(struct task_struct *tsk)
 }
 #endif
 
+#include <linux/livepatch.h>
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
+static int proc_cgroup_mutex_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	struct mutex *mutex = (struct mutex *)shadow_data;
+	mutex_init(mutex);
+	return 0;
+}
+extern void proc_cgroup_cache_clear(struct task_struct *tsk);
 void free_task(struct task_struct *tsk)
 {
 	scs_release(tsk);
@@ -461,6 +474,10 @@ void free_task(struct task_struct *tsk)
 	arch_release_task_struct(tsk);
 	if (tsk->flags & PF_KTHREAD)
 		free_kthread_struct(tsk);
+	proc_cgroup_cache_clear(tsk);
+	klp_shadow_free(tsk, SHADOW_MUTEX, NULL);
+	klp_shadow_free(tsk, SHADOW_CACHE, NULL);
+	klp_shadow_free(tsk, SHADOW_KEY, NULL);
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);
@@ -1955,6 +1972,14 @@ static __latent_entropy struct task_struct *copy_process(
 	if (!p)
 		goto fork_out;
 
+#ifdef CONFIG_CGROUPS
+	klp_shadow_get_or_alloc(p, SHADOW_CACHE,
+	    sizeof(void *) * 16, GFP_KERNEL, NULL, NULL);
+	klp_shadow_get_or_alloc(p, SHADOW_KEY,
+	    sizeof(void *) * 16, GFP_KERNEL, NULL, NULL);
+	klp_shadow_get_or_alloc(p, SHADOW_MUTEX,
+	    sizeof(struct mutex), GFP_KERNEL, proc_cgroup_mutex_ctor, NULL);
+#endif
 	/*
 	 * This _must_ happen before we call free_task(), i.e. before we jump
 	 * to any of the bad_fork_* labels. This is to avoid freeing
diff --git a/kernel/sched/psi.c b/kernel/sched/psi.c
index b7f38f3ad42a..88fef13cf928 100644
--- a/kernel/sched/psi.c
+++ b/kernel/sched/psi.c
@@ -955,6 +955,12 @@ void psi_cgroup_free(struct cgroup *cgroup)
 	WARN_ONCE(cgroup->psi.poll_states, "psi: trigger leak\n");
 }
 
+#include <linux/livepatch.h>
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
 /**
  * cgroup_move_task - move task to a different cgroup
  * @task: the task
@@ -972,6 +978,7 @@ void cgroup_move_task(struct task_struct *task, struct css_set *to)
 	unsigned int task_flags = 0;
 	struct rq_flags rf;
 	struct rq *rq;
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(task, SHADOW_MUTEX);
 
 	if (static_branch_likely(&psi_disabled)) {
 		/*
@@ -984,6 +991,12 @@ void cgroup_move_task(struct task_struct *task, struct css_set *to)
 
 	rq = task_rq_lock(task, &rf);
 
+	if (proc_cgroup_mutex) {
+		mutex_lock(proc_cgroup_mutex);
+		proc_cgroup_cache_clear(task);
+		mutex_unlock(proc_cgroup_mutex);
+	}
+
 	if (task_on_rq_queued(task)) {
 		task_flags = TSK_RUNNING;
 		if (task_current(rq, task))
diff --git a/kernel/vpsadminos.c b/kernel/vpsadminos.c
index 4be1c38461ee..2cfa8fba0c08 100644
--- a/kernel/vpsadminos.c
+++ b/kernel/vpsadminos.c
@@ -9,6 +9,74 @@
 #include <linux/xarray.h>
 #include <asm/page.h>
 #include "sched/sched.h"
+#include <linux/vpsadminos-livepatch.h>
+#include "kpatch-macros.h"
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
+static int proc_cgroup_mutex_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	struct mutex *mutex = (struct mutex *)shadow_data;
+	mutex_init(mutex);
+	return 0;
+}
+
+extern struct mutex cgroup_mutex;
+char old_uname[65];
+char new_uname[65];
+static int patch(patch_object *obj)
+{
+	struct task_struct *p, *t;
+	mutex_lock(&cgroup_mutex);
+	read_lock(&tasklist_lock);
+	for_each_process_thread(p, t) {
+		task_lock(t);
+		klp_shadow_get_or_alloc(t, SHADOW_CACHE,
+		    sizeof(void *) * 16, GFP_KERNEL, NULL, NULL);
+		klp_shadow_get_or_alloc(t, SHADOW_KEY,
+		    sizeof(void *) * 16, GFP_KERNEL, NULL, NULL);
+		klp_shadow_get_or_alloc(t, SHADOW_MUTEX,
+		    sizeof(struct mutex), GFP_KERNEL, proc_cgroup_mutex_ctor, NULL);
+		task_unlock(t);
+	};
+	read_unlock(&tasklist_lock);
+	mutex_unlock(&cgroup_mutex);
+	scnprintf(new_uname, 64, "%s.%s", LIVEPATCH_ORIG_KERNEL_VERSION,
+	    LIVEPATCH_NAME);
+	scnprintf(old_uname, 64, "%s", init_uts_ns.name.release);
+	scnprintf(init_uts_ns.name.release, 64, "%s", new_uname);
+	return 0;
+}
+KPATCH_PRE_PATCH_CALLBACK(patch);
+extern void proc_cgroup_cache_clear(struct task_struct *tsk);
+extern struct mutex *show_cpuinfo_cache_mutexes[NR_CPUS];
+static void unpatch(patch_object *obj)
+{
+	struct task_struct *p, *t;
+	int cpu;
+	for_each_online_cpu(cpu)
+		if (show_cpuinfo_cache_mutexes[cpu])
+			kfree(show_cpuinfo_cache_mutexes[cpu]);
+	mutex_lock(&cgroup_mutex);
+	read_lock(&tasklist_lock);
+	for_each_process_thread(p, t) {
+		struct mutex *m = klp_shadow_get(t, SHADOW_MUTEX);
+		if (m) {
+			mutex_lock(m);
+			proc_cgroup_cache_clear(t);
+			mutex_unlock(m);
+		};
+	};
+	klp_shadow_free_all(SHADOW_MUTEX, NULL);
+	klp_shadow_free_all(SHADOW_CACHE, NULL);
+	klp_shadow_free_all(SHADOW_KEY, NULL);
+	read_unlock(&tasklist_lock);
+	mutex_unlock(&cgroup_mutex);
+	scnprintf(init_uts_ns.name.release, 64, "%s", old_uname);
+}
+KPATCH_POST_UNPATCH_CALLBACK(unpatch);
 
 int online_cpus_in_cpu_cgroup(struct task_struct *p)
 {
diff --git a/net/core/fib_rules.c b/net/core/fib_rules.c
index a6159624eec0..c26427302c3e 100644
--- a/net/core/fib_rules.c
+++ b/net/core/fib_rules.c
@@ -323,7 +323,7 @@ int fib_rules_lookup(struct fib_rules_ops *ops, struct flowi *fl,
 		if (!err && ops->suppress && INDIRECT_CALL_MT(ops->suppress,
 							      fib6_rule_suppress,
 							      fib4_rule_suppress,
-							      rule, flags, arg))
+							      rule, 0, arg))
 			continue;
 
 		if (err != -EAGAIN) {
diff --git a/net/ipv6/fib6_rules.c b/net/ipv6/fib6_rules.c
index 3e4c87b29b11..f38353aff8f8 100644
--- a/net/ipv6/fib6_rules.c
+++ b/net/ipv6/fib6_rules.c
@@ -295,7 +295,7 @@ INDIRECT_CALLABLE_SCOPE bool fib6_rule_suppress(struct fib_rule *rule,
 	return false;
 
 suppress_route:
-	ip6_rt_put_flags(rt, flags);
+	ip6_rt_put(rt);
 	return true;
 }
 
diff --git a/net/ipv6/route.c b/net/ipv6/route.c
index cdf215442d37..2d7282c62402 100644
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@ -2471,9 +2471,15 @@ INDIRECT_CALLABLE_SCOPE struct rt6_info *ip6_pol_route_output(struct net *net,
 	return ip6_pol_route(net, table, fl6->flowi6_oif, fl6, skb, flags);
 }
 
-struct dst_entry *ip6_route_output_flags_noref(struct net *net,
-					       const struct sock *sk,
-					       struct flowi6 *fl6, int flags)
+struct dst_entry *ip6_route_output_flags_noref(struct net *net, const struct sock *sk,
+					 struct flowi6 *fl6, int flags)
+{
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(ip6_route_output_flags_noref);
+
+struct dst_entry *ip6_route_output_flags(struct net *net, const struct sock *sk,
+					 struct flowi6 *fl6, int flags)
 {
 	bool any_src;
 
@@ -2481,7 +2487,6 @@ struct dst_entry *ip6_route_output_flags_noref(struct net *net,
 	    (IPV6_ADDR_MULTICAST | IPV6_ADDR_LINKLOCAL)) {
 		struct dst_entry *dst;
 
-		/* This function does not take refcnt on the dst */
 		dst = l3mdev_link_scope_lookup(net, fl6);
 		if (dst)
 			return dst;
@@ -2489,7 +2494,6 @@ struct dst_entry *ip6_route_output_flags_noref(struct net *net,
 
 	fl6->flowi6_iif = LOOPBACK_IFINDEX;
 
-	flags |= RT6_LOOKUP_F_DST_NOREF;
 	any_src = ipv6_addr_any(&fl6->saddr);
 	if ((sk && sk->sk_bound_dev_if) || rt6_need_strict(&fl6->daddr) ||
 	    (fl6->flowi6_oif && any_src))
@@ -2502,28 +2506,6 @@ struct dst_entry *ip6_route_output_flags_noref(struct net *net,
 
 	return fib6_rule_lookup(net, fl6, NULL, flags, ip6_pol_route_output);
 }
-EXPORT_SYMBOL_GPL(ip6_route_output_flags_noref);
-
-struct dst_entry *ip6_route_output_flags(struct net *net,
-					 const struct sock *sk,
-					 struct flowi6 *fl6,
-					 int flags)
-{
-        struct dst_entry *dst;
-        struct rt6_info *rt6;
-
-        rcu_read_lock();
-        dst = ip6_route_output_flags_noref(net, sk, fl6, flags);
-        rt6 = (struct rt6_info *)dst;
-        /* For dst cached in uncached_list, refcnt is already taken. */
-        if (list_empty(&rt6->rt6i_uncached) && !dst_hold_safe(dst)) {
-                dst = &net->ipv6.ip6_null_entry->dst;
-                dst_hold(dst);
-        }
-        rcu_read_unlock();
-
-        return dst;
-}
 EXPORT_SYMBOL_GPL(ip6_route_output_flags);
 
 struct dst_entry *ip6_blackhole_route(struct net *net, struct dst_entry *dst_orig)
diff --git a/net/l3mdev/l3mdev.c b/net/l3mdev/l3mdev.c
index f2c3a61ad134..28fb40ca6b85 100644
--- a/net/l3mdev/l3mdev.c
+++ b/net/l3mdev/l3mdev.c
@@ -211,8 +211,6 @@ EXPORT_SYMBOL_GPL(l3mdev_fib_table_by_index);
  *			     local and multicast addresses
  *	@net: network namespace for device index lookup
  *	@fl6: IPv6 flow struct for lookup
- *	This function does not hold refcnt on the returned dst.
- *	Caller must hold rcu_read_lock().
  */
 
 struct dst_entry *l3mdev_link_scope_lookup(struct net *net,
@@ -221,8 +219,9 @@ struct dst_entry *l3mdev_link_scope_lookup(struct net *net,
 	struct dst_entry *dst = NULL;
 	struct net_device *dev;
 
-	WARN_ON_ONCE(!rcu_read_lock_held());
 	if (fl6->flowi6_oif) {
+		rcu_read_lock();
+
 		dev = dev_get_by_index_rcu(net, fl6->flowi6_oif);
 		if (dev && netif_is_l3_slave(dev))
 			dev = netdev_master_upper_dev_get_rcu(dev);
@@ -230,6 +229,8 @@ struct dst_entry *l3mdev_link_scope_lookup(struct net *net,
 		if (dev && netif_is_l3_master(dev) &&
 		    dev->l3mdev_ops->l3mdev_link_scope_lookup)
 			dst = dev->l3mdev_ops->l3mdev_link_scope_lookup(dev, fl6);
+
+		rcu_read_unlock();
 	}
 
 	return dst;
diff --git a/tools/testing/selftests/net/fib_tests.sh b/tools/testing/selftests/net/fib_tests.sh
index a7f53c2a9580..b6d67109af31 100755
--- a/tools/testing/selftests/net/fib_tests.sh
+++ b/tools/testing/selftests/net/fib_tests.sh
@@ -9,7 +9,7 @@ ret=0
 ksft_skip=4
 
 # all tests in this script. Can be overridden with -t option
-TESTS="unregister down carrier nexthop suppress ipv6_rt ipv4_rt ipv6_addr_metric ipv4_addr_metric ipv6_route_metrics ipv4_route_metrics ipv4_route_v6_gw rp_filter ipv4_del_addr"
+TESTS="unregister down carrier nexthop ipv6_rt ipv4_rt ipv6_addr_metric ipv4_addr_metric ipv6_route_metrics ipv4_route_metrics ipv4_route_v6_gw rp_filter ipv4_del_addr"
 
 VERBOSE=0
 PAUSE_ON_FAIL=no
@@ -655,6 +655,7 @@ fib_nexthop_test()
 	cleanup
 }
 
+<<<<<<< HEAD
 fib_suppress_test()
 {
 	echo
@@ -675,6 +676,8 @@ fib_suppress_test()
 	cleanup
 }
 
+=======
+>>>>>>> parent of ca7a03c41753 (ipv6: do not free rt if FIB_LOOKUP_NOREF is set on suppress rule)
 ################################################################################
 # Tests on route add and replace
 
@@ -1780,7 +1783,6 @@ do
 	fib_carrier_test|carrier)	fib_carrier_test;;
 	fib_rp_filter_test|rp_filter)	fib_rp_filter_test;;
 	fib_nexthop_test|nexthop)	fib_nexthop_test;;
-	fib_suppress_test|suppress)	fib_suppress_test;;
 	ipv6_route_test|ipv6_rt)	ipv6_route_test;;
 	ipv4_route_test|ipv4_rt)	ipv4_route_test;;
 	ipv6_addr_metric)		ipv6_addr_metric_test;;
