diff --git a/arch/x86/kernel/cpu/proc.c b/arch/x86/kernel/cpu/proc.c
index 4eec8889b0ff..f3ea0b35e82b 100644
--- a/arch/x86/kernel/cpu/proc.c
+++ b/arch/x86/kernel/cpu/proc.c
@@ -4,6 +4,7 @@
 #include <linux/string.h>
 #include <linux/seq_file.h>
 #include <linux/cpufreq.h>
+#include <linux/user_namespace.h>
 
 #include "cpu.h"
 
@@ -58,14 +59,52 @@ static void show_cpuinfo_misc(struct seq_file *m, struct cpuinfo_x86 *c)
 }
 #endif
 
+struct mutex *show_cpuinfo_cache_mutexes[NR_CPUS] = { 0 };
+unsigned long show_cpuinfo_cache_jiffies[NR_CPUS] = { 0 };
+char *show_cpuinfo_cache[NR_CPUS] = { 0 };
+
 static int show_cpuinfo(struct seq_file *m, void *v)
 {
 	struct cpuinfo_x86 *c = v;
 	unsigned int cpu;
 	int i;
+	unsigned long now = jiffies;
+	struct seq_file *dupm;
 
 	cpu = c->cpu_index;
-	seq_printf(m, "processor\t: %u\n"
+
+	if (!show_cpuinfo_cache_mutexes[cpu]) {
+		show_cpuinfo_cache_mutexes[cpu] = kmalloc(sizeof(struct mutex), GFP_KERNEL);
+		if (!show_cpuinfo_cache_mutexes[cpu])
+			return -ERESTARTSYS;
+		mutex_init(show_cpuinfo_cache_mutexes[cpu]);
+	}
+
+	mutex_lock(show_cpuinfo_cache_mutexes[cpu]);
+	if (now - show_cpuinfo_cache_jiffies[cpu] > msecs_to_jiffies(5000)) {
+		if (show_cpuinfo_cache[cpu])
+			kfree(show_cpuinfo_cache[cpu]);
+	} else if (show_cpuinfo_cache[cpu]) {
+		seq_puts(m, show_cpuinfo_cache[cpu]);
+		mutex_unlock(show_cpuinfo_cache_mutexes[cpu]);
+		return 0;
+	}
+
+	dupm = kzalloc(sizeof(struct seq_file), GFP_KERNEL);
+	if (!dupm) {
+		mutex_unlock(show_cpuinfo_cache_mutexes[cpu]);
+		return -ENOMEM;
+	}
+	dupm->buf = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!dupm->buf) {
+		kfree(dupm);
+		mutex_unlock(show_cpuinfo_cache_mutexes[cpu]);
+		return -ENOMEM;
+	}
+	dupm->size = PAGE_SIZE;
+	mutex_init(&dupm->lock);
+
+	seq_printf(dupm, "processor\t: %u\n"
 		   "vendor_id\t: %s\n"
 		   "cpu family\t: %d\n"
 		   "model\t\t: %u\n"
@@ -77,11 +116,11 @@ static int show_cpuinfo(struct seq_file *m, void *v)
 		   c->x86_model_id[0] ? c->x86_model_id : "unknown");
 
 	if (c->x86_stepping || c->cpuid_level >= 0)
-		seq_printf(m, "stepping\t: %d\n", c->x86_stepping);
+		seq_printf(dupm, "stepping\t: %d\n", c->x86_stepping);
 	else
-		seq_puts(m, "stepping\t: unknown\n");
+		seq_puts(dupm, "stepping\t: unknown\n");
 	if (c->microcode)
-		seq_printf(m, "microcode\t: 0x%x\n", c->microcode);
+		seq_printf(dupm, "microcode\t: 0x%x\n", c->microcode);
 
 	if (cpu_has(c, X86_FEATURE_TSC)) {
 		unsigned int freq = aperfmperf_get_khz(cpu);
@@ -90,69 +129,81 @@ static int show_cpuinfo(struct seq_file *m, void *v)
 			freq = cpufreq_quick_get(cpu);
 		if (!freq)
 			freq = cpu_khz;
-		seq_printf(m, "cpu MHz\t\t: %u.%03u\n",
+		seq_printf(dupm, "cpu MHz\t\t: %u.%03u\n",
 			   freq / 1000, (freq % 1000));
 	}
 
 	/* Cache size */
 	if (c->x86_cache_size)
-		seq_printf(m, "cache size\t: %u KB\n", c->x86_cache_size);
+		seq_printf(dupm, "cache size\t: %u KB\n", c->x86_cache_size);
 
-	show_cpuinfo_core(m, c, cpu);
-	show_cpuinfo_misc(m, c);
+	show_cpuinfo_core(dupm, c, cpu);
+	show_cpuinfo_misc(dupm, c);
 
-	seq_puts(m, "flags\t\t:");
+	seq_puts(dupm, "flags\t\t:");
 	for (i = 0; i < 32*NCAPINTS; i++)
 		if (cpu_has(c, i) && x86_cap_flags[i] != NULL)
-			seq_printf(m, " %s", x86_cap_flags[i]);
+			seq_printf(dupm, " %s", x86_cap_flags[i]);
 
 #ifdef CONFIG_X86_VMX_FEATURE_NAMES
 	if (cpu_has(c, X86_FEATURE_VMX) && c->vmx_capability[0]) {
-		seq_puts(m, "\nvmx flags\t:");
+		seq_puts(dupm, "\nvmx flags\t:");
 		for (i = 0; i < 32*NVMXINTS; i++) {
 			if (test_bit(i, (unsigned long *)c->vmx_capability) &&
 			    x86_vmx_flags[i] != NULL)
-				seq_printf(m, " %s", x86_vmx_flags[i]);
+				seq_printf(dupm, " %s", x86_vmx_flags[i]);
 		}
 	}
 #endif
 
-	seq_puts(m, "\nbugs\t\t:");
+	seq_puts(dupm, "\nbugs\t\t:");
 	for (i = 0; i < 32*NBUGINTS; i++) {
 		unsigned int bug_bit = 32*NCAPINTS + i;
 
 		if (cpu_has_bug(c, bug_bit) && x86_bug_flags[i])
-			seq_printf(m, " %s", x86_bug_flags[i]);
+			seq_printf(dupm, " %s", x86_bug_flags[i]);
 	}
 
-	seq_printf(m, "\nbogomips\t: %lu.%02lu\n",
+	seq_printf(dupm, "\nbogomips\t: %lu.%02lu\n",
 		   c->loops_per_jiffy/(500000/HZ),
 		   (c->loops_per_jiffy/(5000/HZ)) % 100);
 
 #ifdef CONFIG_X86_64
 	if (c->x86_tlbsize > 0)
-		seq_printf(m, "TLB size\t: %d 4K pages\n", c->x86_tlbsize);
+		seq_printf(dupm, "TLB size\t: %d 4K pages\n", c->x86_tlbsize);
 #endif
-	seq_printf(m, "clflush size\t: %u\n", c->x86_clflush_size);
-	seq_printf(m, "cache_alignment\t: %d\n", c->x86_cache_alignment);
-	seq_printf(m, "address sizes\t: %u bits physical, %u bits virtual\n",
+	seq_printf(dupm, "clflush size\t: %u\n", c->x86_clflush_size);
+	seq_printf(dupm, "cache_alignment\t: %d\n", c->x86_cache_alignment);
+	seq_printf(dupm, "address sizes\t: %u bits physical, %u bits virtual\n",
 		   c->x86_phys_bits, c->x86_virt_bits);
 
-	seq_puts(m, "power management:");
+	seq_puts(dupm, "power management:");
 	for (i = 0; i < 32; i++) {
 		if (c->x86_power & (1 << i)) {
 			if (i < ARRAY_SIZE(x86_power_flags) &&
 			    x86_power_flags[i])
-				seq_printf(m, "%s%s",
+				seq_printf(dupm, "%s%s",
 					   x86_power_flags[i][0] ? " " : "",
 					   x86_power_flags[i]);
 			else
-				seq_printf(m, " [%d]", i);
+				seq_printf(dupm, " [%d]", i);
 		}
 	}
 
-	seq_puts(m, "\n\n");
+	seq_puts(dupm, "\n\n");
 
+	if (dupm->buf) {
+		show_cpuinfo_cache[cpu] = kzalloc(dupm->count + 1, GFP_KERNEL);
+		if (show_cpuinfo_cache[cpu])
+			memcpy(show_cpuinfo_cache[cpu], dupm->buf, dupm->count);
+		seq_puts(m, dupm->buf);
+		kfree(dupm->buf);
+	}
+	if (show_cpuinfo_cache[cpu]) {
+		show_cpuinfo_cache_jiffies[cpu] = now;
+	}
+	kfree(dupm);
+	mutex_unlock(show_cpuinfo_cache_mutexes[cpu]);
 	return 0;
 }
 
diff --git a/fs/pipe.c b/fs/pipe.c
index a2fb61e0d04e..dbb090e1b026 100644
--- a/fs/pipe.c
+++ b/fs/pipe.c
@@ -1302,8 +1302,6 @@ int pipe_resize_ring(struct pipe_inode_info *pipe, unsigned int nr_slots)
 	pipe->tail = tail;
 	pipe->head = head;
 
-	pipe->max_usage = nr_slots;
-	pipe->nr_accounted = nr_slots;
 	spin_unlock_irq(&pipe->rd_wait.lock);
 
 	/* This might have made more room for writers */
@@ -1357,6 +1355,8 @@ static long pipe_set_size(struct pipe_inode_info *pipe, unsigned long arg)
 	if (ret < 0)
 		goto out_revert_acct;
 
+	pipe->max_usage = nr_slots;
+	pipe->nr_accounted = nr_slots;
 	return pipe->max_usage * PAGE_SIZE;
 
 out_revert_acct:
diff --git a/fs/proc/cpuinfo.c b/fs/proc/cpuinfo.c
index 419760fd77bd..7a42ced1aa45 100644
--- a/fs/proc/cpuinfo.c
+++ b/fs/proc/cpuinfo.c
@@ -9,10 +9,19 @@ __weak void arch_freq_prepare_all(void)
 {
 }
 
+extern unsigned long show_cpuinfo_cache_jiffies[NR_CPUS];
 extern const struct seq_operations cpuinfo_op;
 static int cpuinfo_open(struct inode *inode, struct file *file)
 {
-	arch_freq_prepare_all();
+	unsigned long now = jiffies;
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		if (now - show_cpuinfo_cache_jiffies[cpu] > msecs_to_jiffies(5000)) {
+			arch_freq_prepare_all();
+			break;
+		}
+	}
 	return seq_open(file, &cpuinfo_op);
 }
 
diff --git a/kernel/cgroup/cgroup.c b/kernel/cgroup/cgroup.c
index a4325ebc6979..bb58d3041349 100644
--- a/kernel/cgroup/cgroup.c
+++ b/kernel/cgroup/cgroup.c
@@ -68,6 +68,8 @@
 /* let's not notify more than 100 times per second */
 #define CGROUP_FILE_NOTIFY_MIN_INTV	DIV_ROUND_UP(HZ, 100)
 
+void proc_cgroup_cache_clear(struct task_struct *tsk);
+
 /*
  * cgroup_mutex is the master lock.  Any modification to cgroup or its
  * hierarchy must be performed while holding it.
@@ -855,6 +857,12 @@ static void css_set_skip_task_iters(struct css_set *cset,
 		css_task_iter_skip(it, task);
 }
 
+#include <linux/livepatch.h>
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
 /**
  * css_set_move_task - move a task from one css_set to another
  * @task: task being moved
@@ -5955,6 +5963,71 @@ void cgroup_path_from_kernfs_id(u64 id, char *buf, size_t buflen)
 	kernfs_put(kn);
 }
 
+/* Needs tsk->proc_cgroup_mutex */
+void proc_cgroup_cache_clear(struct task_struct *tsk)
+{
+	struct mutex *mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+
+	if (!mutex || !caches || !keys)
+		return;
+
+	for (i = 0; i < 16; i++) {
+		if (keys[i] != 0) {
+			keys[i] = 0;
+			if (caches[i])
+				kfree(caches[i]);
+		};
+	};
+}
+
+/* Needs tsk->proc_cgroup_mutex */
+char *proc_cgroup_cache_lookup(struct task_struct *tsk, struct cgroup_namespace *srchkey)
+{
+	struct mutex *mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+
+	if (!mutex || !caches || !keys)
+		return NULL;
+
+	for (i = 0; i < 16; i++) {
+		if (keys[i] == srchkey)
+			return caches[i];
+	};
+	return NULL;
+}
+
+/* Needs tsk->proc_cgroup_mutex */
+char *proc_cgroup_cache_alloc(struct task_struct *tsk, struct cgroup_namespace *srchkey, char* buf, size_t len)
+{
+	struct mutex *mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+	char *ret;
+
+	if (!mutex || !caches || !keys)
+		return NULL;
+
+	for (i = 0; i < 16; i++) {
+		if (!keys[i]) {
+			ret = kzalloc(len+1, GFP_KERNEL);
+			if (!ret)
+				return NULL;
+			caches[i] = ret;
+			keys[i] = srchkey;
+			memcpy(ret, buf, len);
+			return ret;
+		};
+	};
+	proc_cgroup_cache_clear(tsk);
+	return NULL;
+}
+
 /*
  * proc_cgroup_show()
  *  - Print task's cgroup paths into seq_file, one line for each hierarchy
@@ -5963,11 +6036,37 @@ void cgroup_path_from_kernfs_id(u64 id, char *buf, size_t buflen)
 int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 		     struct pid *pid, struct task_struct *tsk)
 {
+	char *cache;
 	char *buf;
-	int retval;
+	int retval = -ENOMEM;
 	struct cgroup_root *root;
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+	struct seq_file *dupm;
+
+	if (!proc_cgroup_mutex)
+		goto orig;
+	mutex_lock(proc_cgroup_mutex);
+	cache = proc_cgroup_cache_lookup(tsk, current->nsproxy->cgroup_ns);
+	if (cache) {
+		seq_puts(m, cache);
+		mutex_unlock(proc_cgroup_mutex);
+		return 0;
+	}
 
-	retval = -ENOMEM;
+	dupm = kzalloc(sizeof(struct seq_file), GFP_KERNEL);
+	if (!dupm) {
+		mutex_unlock(proc_cgroup_mutex);
+		return -ENOMEM;
+	}
+	dupm->buf = kzalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!dupm->buf) {
+		kfree(dupm);
+		mutex_unlock(proc_cgroup_mutex);
+		return -ENOMEM;
+	}
+	dupm->size = PAGE_SIZE;
+	mutex_init(&dupm->lock);
+orig:
 	buf = kmalloc(PATH_MAX, GFP_KERNEL);
 	if (!buf)
 		goto out;
@@ -5983,16 +6082,16 @@ int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 		if (root == &cgrp_dfl_root && !cgrp_dfl_visible)
 			continue;
 
-		seq_printf(m, "%d:", root->hierarchy_id);
+		seq_printf(dupm, "%d:", root->hierarchy_id);
 		if (root != &cgrp_dfl_root)
 			for_each_subsys(ss, ssid)
 				if (root->subsys_mask & (1 << ssid))
-					seq_printf(m, "%s%s", count++ ? "," : "",
+					seq_printf(dupm, "%s%s", count++ ? "," : "",
 						   ss->legacy_name);
 		if (strlen(root->name))
-			seq_printf(m, "%sname=%s", count ? "," : "",
+			seq_printf(dupm, "%sname=%s", count ? "," : "",
 				   root->name);
-		seq_putc(m, ':');
+		seq_putc(dupm, ':');
 
 		cgrp = task_cgroup_from_root(tsk, root);
 
@@ -6013,15 +6112,15 @@ int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 			if (retval < 0)
 				goto out_unlock;
 
-			seq_puts(m, buf);
+			seq_puts(dupm, buf);
 		} else {
-			seq_puts(m, "/");
+			seq_puts(dupm, "/");
 		}
 
 		if (cgroup_on_dfl(cgrp) && cgroup_is_dead(cgrp))
-			seq_puts(m, " (deleted)\n");
+			seq_puts(dupm, " (deleted)\n");
 		else
-			seq_putc(m, '\n');
+			seq_putc(dupm, '\n');
 	}
 
 	retval = 0;
@@ -6030,6 +6129,17 @@ int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 	mutex_unlock(&cgroup_mutex);
 	kfree(buf);
 out:
+	if (!proc_cgroup_mutex)
+		goto out_orig;
+	if (dupm->buf) {
+		cache = proc_cgroup_cache_alloc(tsk, current->nsproxy->cgroup_ns, dupm->buf, dupm->count);
+		kfree(dupm->buf);
+	}
+	if (cache)
+		seq_puts(m, cache);
+	kfree(dupm);
+	mutex_unlock(proc_cgroup_mutex);
+out_orig:
 	return retval;
 }
 
diff --git a/kernel/fork.c b/kernel/fork.c
index 6a060869f94c..1444f458bfa7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -439,6 +439,19 @@ void put_task_stack(struct task_struct *tsk)
 }
 #endif
 
+#include <linux/livepatch.h>
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
+static int proc_cgroup_mutex_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	struct mutex *mutex = (struct mutex *)shadow_data;
+	mutex_init(mutex);
+	return 0;
+}
+extern void proc_cgroup_cache_clear(struct task_struct *tsk);
 void free_task(struct task_struct *tsk)
 {
 	scs_release(tsk);
@@ -461,6 +474,10 @@ void free_task(struct task_struct *tsk)
 	arch_release_task_struct(tsk);
 	if (tsk->flags & PF_KTHREAD)
 		free_kthread_struct(tsk);
+	proc_cgroup_cache_clear(tsk);
+	klp_shadow_free(tsk, SHADOW_MUTEX, NULL);
+	klp_shadow_free(tsk, SHADOW_CACHE, NULL);
+	klp_shadow_free(tsk, SHADOW_KEY, NULL);
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);
@@ -1955,6 +1972,14 @@ static __latent_entropy struct task_struct *copy_process(
 	if (!p)
 		goto fork_out;
 
+#ifdef CONFIG_CGROUPS
+	klp_shadow_get_or_alloc(p, SHADOW_CACHE,
+	    sizeof(void *) * 16, GFP_KERNEL, NULL, NULL);
+	klp_shadow_get_or_alloc(p, SHADOW_KEY,
+	    sizeof(void *) * 16, GFP_KERNEL, NULL, NULL);
+	klp_shadow_get_or_alloc(p, SHADOW_MUTEX,
+	    sizeof(struct mutex), GFP_KERNEL, proc_cgroup_mutex_ctor, NULL);
+#endif
 	/*
 	 * This _must_ happen before we call free_task(), i.e. before we jump
 	 * to any of the bad_fork_* labels. This is to avoid freeing
diff --git a/kernel/sched/psi.c b/kernel/sched/psi.c
index b7f38f3ad42a..88fef13cf928 100644
--- a/kernel/sched/psi.c
+++ b/kernel/sched/psi.c
@@ -955,6 +955,12 @@ void psi_cgroup_free(struct cgroup *cgroup)
 	WARN_ONCE(cgroup->psi.poll_states, "psi: trigger leak\n");
 }
 
+#include <linux/livepatch.h>
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
 /**
  * cgroup_move_task - move task to a different cgroup
  * @task: the task
@@ -972,6 +978,7 @@ void cgroup_move_task(struct task_struct *task, struct css_set *to)
 	unsigned int task_flags = 0;
 	struct rq_flags rf;
 	struct rq *rq;
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(task, SHADOW_MUTEX);
 
 	if (static_branch_likely(&psi_disabled)) {
 		/*
@@ -984,6 +991,12 @@ void cgroup_move_task(struct task_struct *task, struct css_set *to)
 
 	rq = task_rq_lock(task, &rf);
 
+	if (proc_cgroup_mutex) {
+		mutex_lock(proc_cgroup_mutex);
+		proc_cgroup_cache_clear(task);
+		mutex_unlock(proc_cgroup_mutex);
+	}
+
 	if (task_on_rq_queued(task)) {
 		task_flags = TSK_RUNNING;
 		if (task_current(rq, task))
diff --git a/kernel/vpsadminos.c b/kernel/vpsadminos.c
index 4be1c38461ee..2cfa8fba0c08 100644
--- a/kernel/vpsadminos.c
+++ b/kernel/vpsadminos.c
@@ -9,6 +9,74 @@
 #include <linux/xarray.h>
 #include <asm/page.h>
 #include "sched/sched.h"
+#include <linux/vpsadminos-livepatch.h>
+#include "kpatch-macros.h"
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
+static int proc_cgroup_mutex_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	struct mutex *mutex = (struct mutex *)shadow_data;
+	mutex_init(mutex);
+	return 0;
+}
+
+extern struct mutex cgroup_mutex;
+char old_uname[65];
+char new_uname[65];
+static int patch(patch_object *obj)
+{
+	struct task_struct *p, *t;
+	mutex_lock(&cgroup_mutex);
+	read_lock(&tasklist_lock);
+	for_each_process_thread(p, t) {
+		task_lock(t);
+		klp_shadow_get_or_alloc(t, SHADOW_CACHE,
+		    sizeof(void *) * 16, GFP_KERNEL, NULL, NULL);
+		klp_shadow_get_or_alloc(t, SHADOW_KEY,
+		    sizeof(void *) * 16, GFP_KERNEL, NULL, NULL);
+		klp_shadow_get_or_alloc(t, SHADOW_MUTEX,
+		    sizeof(struct mutex), GFP_KERNEL, proc_cgroup_mutex_ctor, NULL);
+		task_unlock(t);
+	};
+	read_unlock(&tasklist_lock);
+	mutex_unlock(&cgroup_mutex);
+	scnprintf(new_uname, 64, "%s.%s", LIVEPATCH_ORIG_KERNEL_VERSION,
+	    LIVEPATCH_NAME);
+	scnprintf(old_uname, 64, "%s", init_uts_ns.name.release);
+	scnprintf(init_uts_ns.name.release, 64, "%s", new_uname);
+	return 0;
+}
+KPATCH_PRE_PATCH_CALLBACK(patch);
+extern void proc_cgroup_cache_clear(struct task_struct *tsk);
+extern struct mutex *show_cpuinfo_cache_mutexes[NR_CPUS];
+static void unpatch(patch_object *obj)
+{
+	struct task_struct *p, *t;
+	int cpu;
+	for_each_online_cpu(cpu)
+		if (show_cpuinfo_cache_mutexes[cpu])
+			kfree(show_cpuinfo_cache_mutexes[cpu]);
+	mutex_lock(&cgroup_mutex);
+	read_lock(&tasklist_lock);
+	for_each_process_thread(p, t) {
+		struct mutex *m = klp_shadow_get(t, SHADOW_MUTEX);
+		if (m) {
+			mutex_lock(m);
+			proc_cgroup_cache_clear(t);
+			mutex_unlock(m);
+		};
+	};
+	klp_shadow_free_all(SHADOW_MUTEX, NULL);
+	klp_shadow_free_all(SHADOW_CACHE, NULL);
+	klp_shadow_free_all(SHADOW_KEY, NULL);
+	read_unlock(&tasklist_lock);
+	mutex_unlock(&cgroup_mutex);
+	scnprintf(init_uts_ns.name.release, 64, "%s", old_uname);
+}
+KPATCH_POST_UNPATCH_CALLBACK(unpatch);
 
 int online_cpus_in_cpu_cgroup(struct task_struct *p)
 {
